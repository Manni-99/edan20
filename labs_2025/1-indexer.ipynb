{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1: Building an inverted index\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program that collects all the words from a set of documents\n",
    "* Build an index from the words\n",
    "* Represent a document using the Tf.Idf values\n",
    "* Write a short report of 1 to 2 pages on the assignment\n",
    "* Read a description of an industrial system and answer a question on it\n",
    "* Discuss the potential of corpora in applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the missing code and run all the cells, you will submit your notebook to an automatic marking system. Do not erase the content of the cells as we will possibly check your programs manually.\n",
    "The submission instructions are at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build an indexer to index all the words in a corpus. Conceptually, an index consists of rows with one word per row and the list of files and positions, where this word occurs. Such a row is called a _posting list_. You will encode the position of a word by the number of characters from the start of the file.\n",
    "<pre>\n",
    "word1: file_name pos1 pos2 pos3... file_name pos1 pos2 ...\n",
    "word2: file_name pos1 pos2 pos3... file_name pos1 pos2 ...\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports. Add others as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create an index for a corpus of Selma Lagerlöf's works: To gather the corpus, you can alternatively:\n",
    "1. Download the <a href=\"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\">Selma folder</a> and uncompress it. It contains novels by <a href=\"https://sv.wikipedia.org/wiki/Selma_Lagerl%C3%B6f\">Selma Lagerlöf</a>. The text of these novels was extracted from <a href=\"https://litteraturbanken.se/forfattare/LagerlofS/titlar\">Lagerlöf arkivet</a> at <a href=\"https://litteraturbanken.se/\">Litteraturbanken</a>.\n",
    "2. Or run this cell that will download the corpus and place it in your folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 65536 bytes written to Selma.zip\n",
      "Downloading: 131072 bytes written to Selma.zip\n",
      "Downloading: 196608 bytes written to Selma.zip\n",
      "Downloading: 262144 bytes written to Selma.zip\n",
      "Downloading: 327680 bytes written to Selma.zip\n",
      "Downloading: 393216 bytes written to Selma.zip\n",
      "Downloading: 458752 bytes written to Selma.zip\n",
      "Downloading: 524288 bytes written to Selma.zip\n",
      "Downloading: 589824 bytes written to Selma.zip\n",
      "Downloading: 655360 bytes written to Selma.zip\n",
      "Downloading: 720896 bytes written to Selma.zip\n",
      "Downloading: 786432 bytes written to Selma.zip\n",
      "Downloading: 851968 bytes written to Selma.zip\n",
      "Downloading: 917504 bytes written to Selma.zip\n",
      "Downloading: 983040 bytes written to Selma.zip\n",
      "Downloading: 1048576 bytes written to Selma.zip\n",
      "Downloading: 1114112 bytes written to Selma.zip\n",
      "Downloading: 1179648 bytes written to Selma.zip\n",
      "Downloading: 1245184 bytes written to Selma.zip\n",
      "Downloading: 1310720 bytes written to Selma.zip\n",
      "Downloading: 1376256 bytes written to Selma.zip\n",
      "Downloading: 1441792 bytes written to Selma.zip\n",
      "Downloading: 1507328 bytes written to Selma.zip\n",
      "Downloading: 1572864 bytes written to Selma.zip\n",
      "Downloading: 1638400 bytes written to Selma.zip\n",
      "Downloading: 1703936 bytes written to Selma.zip\n",
      "Downloading: 1769472 bytes written to Selma.zip\n",
      "Downloading: 1835008 bytes written to Selma.zip\n",
      "Downloading: 1900544 bytes written to Selma.zip\n",
      "Downloading: 1941636 bytes written to Selma.zip\n",
      "Selma.zip donwnloaded.\n",
      "Extracted: Selma/troll.txt\n",
      "Extracted: Selma/kejsaren.txt\n",
      "Extracted: Selma/marbacka.txt\n",
      "Extracted: Selma/herrgard.txt\n",
      "Extracted: Selma/nils.txt\n",
      "Extracted: Selma/osynliga.txt\n",
      "Extracted: Selma/jerusalem.txt\n",
      "Extracted: Selma/bannlyst.txt\n",
      "Extracted: Selma/gosta.txt\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selma/bannlyst.txt',\n",
       " 'Selma/gosta.txt',\n",
       " 'Selma/herrgard.txt',\n",
       " 'Selma/jerusalem.txt',\n",
       " 'Selma/kejsaren.txt',\n",
       " 'Selma/marbacka.txt',\n",
       " 'Selma/nils.txt',\n",
       " 'Selma/osynliga.txt',\n",
       " 'Selma/troll.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname) \n",
    "    for fname in \n",
    "    [\n",
    "        \"bannlyst.txt\", \n",
    "        \"gosta.txt\", \n",
    "        \"herrgard.txt\", \n",
    "        \"jerusalem.txt\", \n",
    "        \"kejsaren.txt\", \n",
    "        \"marbacka.txt\", \n",
    "        \"nils.txt\", \n",
    "        \"osynliga.txt\", \n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "    \n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "        \n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "            \n",
    "        print(\"Done!\")\n",
    "        \n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "    \n",
    "SELMA_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the indexer (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a production context, your final program would take a corpus as input (here the Selma Lagerlöf's novels) and create an index of all the words with their positions. You should be able to run it this way:\n",
    "<pre>$ python indexer.py folder_name</pre>\n",
    "In this lab, you will write the index in a Jupyter Notebook. The conversion into a Python program is left as an optional exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming the Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make programming easier, you will split this exercise into five steps:\n",
    "1. Index one file;\n",
    "2. Read the content of a folder\n",
    "3. Create a master index for all the files\n",
    "4. Use tfidf to represent the documents (novels)\n",
    "5. Compare the documents of a collection\n",
    "\n",
    "You will use dictionaries to represent the postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Write a program that reads one document <tt>file_name.txt</tt> and outputs an index file:\n",
    "            <tt>file_name.idx</tt>:\n",
    "        </p>\n",
    "        <ol>\n",
    "            <li>The index file will contain all the unique words in the document,\n",
    "                where each word is associated with the list of its positions in the document.\n",
    "            </li>\n",
    "            <li>You will represent this index as a dictionary, where the keys will be the words, and\n",
    "                the values, the lists of positions\n",
    "            </li>\n",
    "            <li>As words, you will consider all the strings of letters that you will set in lower case.\n",
    "                You will not index the rest (i.e. numbers, punctuations, or symbols).\n",
    "            </li>\n",
    "            <li>To extract the words, use Unicode regular expressions. Do not use <tt>\\w+</tt>,\n",
    "                for instance, but the Unicode equivalent.\n",
    "            </li>\n",
    "            <li>The word positions will correspond to the number of characters from the beginning of the file.\n",
    "                (The word offset from the beginning)\n",
    "            </li>\n",
    "            <li>You will use the <tt>finditer()</tt> method to find the positions of the words.\n",
    "                This will return you match objects,\n",
    "                where you will get the matches and the positions with\n",
    "                the <tt>group()</tt> and <tt>start()</tt> methods.\n",
    "            </li>\n",
    "            <li>You will use the pickle package to write your dictionary in an file,\n",
    "                see <a href=\"https://wiki.python.org/moin/UsingPickle\">https://wiki.python.org/moin/UsingPickle</a>.\n",
    "            </li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an excerpt of the index of the `bannlyst.txt` text for the words <i>gjord</i>, <i>uppklarnande</i>, and <i>stjärnor</i>. The data is stored in a dictionary:\n",
    "\n",
    "<pre>\n",
    "{...\n",
    "'gjord': [8600, 183039, 220445],\n",
    "'uppklarnande': [8617],\n",
    "'stjärnor': [8641], ...\n",
    "}\n",
    "</pre>\n",
    "where the word <i>gjord</i> occurs three times in the text at positions 8600, 183039, and 220445, <i>uppklarnande</i>, once at position 8617, and <i>stjärnor</i>, once at position 8641."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a **Unicode** regular expression to find words defined as sequences of letters. Using the Unicode notation is compulsory. If you are not sure, please refer to the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your regex here\n",
    "regex = \"\\\\p{L}+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En',\n",
       " 'gång',\n",
       " 'hade',\n",
       " 'de',\n",
       " 'på',\n",
       " 'Mårbacka',\n",
       " 'en',\n",
       " 'barnpiga',\n",
       " 'som',\n",
       " 'hette',\n",
       " 'Back',\n",
       " 'Kajsa']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(regex, 'En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `regex`, write `tokenize(text)` function to tokenize a text. Return their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def tokenize(text):\n",
    "    return [(m.group(), m.span()) for m in re.finditer(regex, text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('En', (0, 2)),\n",
       " ('gång', (3, 7)),\n",
       " ('hade', (8, 12)),\n",
       " ('de', (13, 15)),\n",
       " ('på', (16, 18)),\n",
       " ('Mårbacka', (19, 27)),\n",
       " ('en', (28, 30)),\n",
       " ('barnpiga', (31, 39)),\n",
       " ('som', (41, 44)),\n",
       " ('hette', (45, 50)),\n",
       " ('Back', (51, 55)),\n",
       " ('Kajsa', (56, 61))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize('En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa.')\n",
    "list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `text_to_idx(words)` function to extract the indices from the list of tokens (words). Return a dictionary, where the keys will be the tokens (words), and the values a list of positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def text_to_idx(words):\n",
    "    index = {}\n",
    "    for i, (word, span) in enumerate(words):\n",
    "        if word not in index:\n",
    "            index[word] = []\n",
    "        index[word].append(i)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': [0, 6],\n",
       " 'gång': [1],\n",
       " 'hade': [2],\n",
       " 'de': [3],\n",
       " 'på': [4],\n",
       " 'mårbacka': [5],\n",
       " 'barnpiga': [7],\n",
       " 'som': [8],\n",
       " 'hette': [9],\n",
       " 'back': [10],\n",
       " 'kajsa': [11]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize('En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa.'.lower().strip())\n",
    "text_to_idx(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read one file, _Mårbacka_, `marbacka.txt`, set it in lowercase, tokenize it, and index it. Call this index `idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "with open(\"marbacka.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    marback = f.read()\n",
    "tokens = tokenize(marback.lower().strip())\n",
    "idx = text_to_idx(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 15, 124, 304, 461, 603, 2777, 2828, 5047, 7611, 8840, 9053, 9347, 10806, 11356, 11638, 12059, 12086, 12167, 12170, 12421, 12811, 12950, 13312, 13331, 13391, 13766, 13817, 13861, 13926, 13973, 13997, 14106, 14452, 14659, 14696, 15096, 15174, 15256, 16090, 17433, 17723, 18068, 18896, 18944, 19019, 19546, 19622, 20298, 20469, 20703, 20817, 20864, 20962, 20998, 22007, 22541, 22830, 22857, 23258, 23663, 23834, 23981, 24695, 24743, 24858, 25095, 25379, 25654, 26462, 27309, 27791, 27930, 27983, 28064, 28166, 28483, 28793, 28847, 29040, 29194, 29836, 30298, 30341, 30450, 30675, 31086, 32821, 32884, 33545, 34119, 34377, 34395, 34764, 35392, 36379, 37042, 37297, 37402, 37668, 37791, 39003, 39418, 39666, 39690, 39743, 39806, 40146, 41078, 41091, 41317, 41441, 41565, 41983, 42310, 42451, 43013, 43323, 43407, 43501, 43520, 43832, 44713, 44970, 45077, 45874, 46569, 46801, 46919, 47170, 47611, 48148, 48202, 48292, 48379, 48508, 48629, 48783, 49056, 49158, 49257, 49466, 52035, 53335, 54459, 54668, 54741, 54771, 54798, 54839, 55390, 56362, 57042, 57165, 58277, 58336, 58432, 58695, 58911, 59969, 60514, 61980, 62097, 62216, 62240, 62323, 62373, 62439, 62934, 62972, 63300, 63341, 63384, 63500, 63656, 63826, 64005, 64295, 64561, 64704, 64800, 65136, 65197, 65277, 65469, 65566, 66297, 66893, 67206, 67304]\n"
     ]
    }
   ],
   "source": [
    "print(idx['mårbacka'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your index in a file so that you can reuse it. Use the pickle module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "index_file = 'marbacka.idx'\n",
    "with open(index_file, 'wb') as f:\n",
    "    pickle.dump(idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back your file and store the content in `idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "with open(index_file, 'rb') as f:\n",
    "    idx_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 139,\n",
       " 752,\n",
       " 1700,\n",
       " 2582,\n",
       " 3324,\n",
       " 15117,\n",
       " 15404,\n",
       " 27794,\n",
       " 42175,\n",
       " 49126,\n",
       " 50407,\n",
       " 52053,\n",
       " 60144,\n",
       " 63374,\n",
       " 64910,\n",
       " 67182,\n",
       " 67330,\n",
       " 67799,\n",
       " 67824,\n",
       " 69232,\n",
       " 71328,\n",
       " 72099,\n",
       " 74147,\n",
       " 74255,\n",
       " 74614,\n",
       " 76610,\n",
       " 76884,\n",
       " 77138,\n",
       " 77509,\n",
       " 77787,\n",
       " 77936,\n",
       " 78574,\n",
       " 80597,\n",
       " 81782,\n",
       " 82003,\n",
       " 84363,\n",
       " 84786,\n",
       " 85251,\n",
       " 89837,\n",
       " 97093,\n",
       " 98642,\n",
       " 100474,\n",
       " 105063,\n",
       " 105298,\n",
       " 105721,\n",
       " 108710,\n",
       " 109133,\n",
       " 112844,\n",
       " 113725,\n",
       " 114997,\n",
       " 115583,\n",
       " 115833,\n",
       " 116368,\n",
       " 116557,\n",
       " 121896,\n",
       " 124823,\n",
       " 126409,\n",
       " 126542,\n",
       " 128758,\n",
       " 130976,\n",
       " 131939,\n",
       " 132826,\n",
       " 136914,\n",
       " 137187,\n",
       " 137872,\n",
       " 139196,\n",
       " 140721,\n",
       " 142324,\n",
       " 146781,\n",
       " 151497,\n",
       " 154335,\n",
       " 155139,\n",
       " 155438,\n",
       " 155886,\n",
       " 156405,\n",
       " 158108,\n",
       " 159817,\n",
       " 160107,\n",
       " 161158,\n",
       " 162085,\n",
       " 165847,\n",
       " 168316,\n",
       " 168528,\n",
       " 169111,\n",
       " 170333,\n",
       " 172684,\n",
       " 182047,\n",
       " 182427,\n",
       " 186362,\n",
       " 189535,\n",
       " 190999,\n",
       " 191110,\n",
       " 193177,\n",
       " 196686,\n",
       " 202552,\n",
       " 206340,\n",
       " 207789,\n",
       " 208382,\n",
       " 209874,\n",
       " 210525,\n",
       " 217464,\n",
       " 219933,\n",
       " 221393,\n",
       " 221533,\n",
       " 221880,\n",
       " 222213,\n",
       " 224190,\n",
       " 229501,\n",
       " 229598,\n",
       " 230783,\n",
       " 231453,\n",
       " 232140,\n",
       " 234427,\n",
       " 236193,\n",
       " 236950,\n",
       " 240168,\n",
       " 241891,\n",
       " 242359,\n",
       " 242934,\n",
       " 243030,\n",
       " 244831,\n",
       " 249882,\n",
       " 251277,\n",
       " 251901,\n",
       " 256360,\n",
       " 260244,\n",
       " 261612,\n",
       " 262384,\n",
       " 263856,\n",
       " 266638,\n",
       " 269760,\n",
       " 270113,\n",
       " 270674,\n",
       " 271146,\n",
       " 271885,\n",
       " 272560,\n",
       " 273464,\n",
       " 275086,\n",
       " 275664,\n",
       " 276207,\n",
       " 277407,\n",
       " 292648,\n",
       " 299762,\n",
       " 306277,\n",
       " 307507,\n",
       " 307972,\n",
       " 308148,\n",
       " 308330,\n",
       " 308568,\n",
       " 311856,\n",
       " 317491,\n",
       " 321194,\n",
       " 321925,\n",
       " 328154,\n",
       " 328470,\n",
       " 328977,\n",
       " 330435,\n",
       " 331650,\n",
       " 337494,\n",
       " 340526,\n",
       " 348636,\n",
       " 349331,\n",
       " 350022,\n",
       " 350168,\n",
       " 350674,\n",
       " 350949,\n",
       " 351349,\n",
       " 354175,\n",
       " 354411,\n",
       " 356314,\n",
       " 356541,\n",
       " 356788,\n",
       " 357522,\n",
       " 358413,\n",
       " 359478,\n",
       " 360511,\n",
       " 362108,\n",
       " 363675,\n",
       " 364467,\n",
       " 365049,\n",
       " 366954,\n",
       " 367286,\n",
       " 367741,\n",
       " 368878,\n",
       " 369487,\n",
       " 373757,\n",
       " 377123,\n",
       " 378852,\n",
       " 379696]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the content of a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `get_files(dir, suffix)` function that reads all the files in a folder with a specific `suffix` (txt). You will need the Python `os` package, see <a href=\"https://docs.python.org/3/library/os.html\">https://docs.python.org/3/library/os.html</a>. You will return the file names in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reuse this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marbacka.txt', 'bannlyst.txt', 'jerusalem.txt', 'gosta.txt', 'nils.txt', 'herrgard.txt', 'osynliga.txt', 'troll.txt', 'kejsaren.txt']\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "print(get_files(os.getcwd()+\"/corpus\", \"\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a master index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete your program with the creation of master index, where you will associate each word of the corpus with the files, where it occur and its positions: a posting list\n",
    "Below is an except of the master index with the words <i>samlar</i> and <i>ände</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samlar': {'troll.txt': [641880, 654233],\n",
       "  'nils.txt': [51805, 118943],\n",
       "  'osynliga.txt': [399121],\n",
       "  'gosta.txt': [313784, 409998, 538165]},\n",
       " 'ände': {'troll.txt': [39562, 650112],\n",
       "  'kejsaren.txt': [50171],\n",
       "  'marbacka.txt': [370324],\n",
       "  'nils.txt': [1794],\n",
       "  'osynliga.txt': [272144]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'samlar':\n",
    "            {'troll.txt': [641880, 654233],\n",
    "            'nils.txt': [51805, 118943],\n",
    "            'osynliga.txt': [399121],\n",
    "            'gosta.txt': [313784, 409998, 538165]},\n",
    " 'ände':\n",
    "            {'troll.txt': [39562, 650112],\n",
    "            'kejsaren.txt': [50171],\n",
    "            'marbacka.txt': [370324],\n",
    "            'nils.txt': [1794],\n",
    "            'osynliga.txt': [272144]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word <i>samlar</i>, for instance, occurs three times in the gosta text at positions\n",
    "            313784, 409998, and 538165."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "corpus_folder = os.path.join(os.getcwd(), \"corpus\")\n",
    "corpus_files = get_files(corpus_folder, \"\")  \n",
    "master_index = {}\n",
    "\n",
    "for file_name in corpus_files:\n",
    "    file_path = os.path.join(corpus_folder, file_name)  # full path to the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    for word, start_pos in tokens:\n",
    "        if word not in master_index:\n",
    "            master_index[word] = {}\n",
    "        if file_name not in master_index[word]:  # store using just the filename\n",
    "            master_index[word][file_name] = []\n",
    "        master_index[word][file_name].append(start_pos)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': [641880, 654233],\n",
       " 'nils.txt': [51805, 118943],\n",
       " 'osynliga.txt': [399121],\n",
       " 'gosta.txt': [313784, 409998, 538165]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_index['samlar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': [226291, 387634, 392959],\n",
       " 'marbacka.txt': [16,\n",
       "  139,\n",
       "  752,\n",
       "  1700,\n",
       "  2582,\n",
       "  3324,\n",
       "  15117,\n",
       "  15404,\n",
       "  27794,\n",
       "  42175,\n",
       "  49126,\n",
       "  50407,\n",
       "  52053,\n",
       "  60144,\n",
       "  63374,\n",
       "  64910,\n",
       "  67182,\n",
       "  67330,\n",
       "  67799,\n",
       "  67824,\n",
       "  69232,\n",
       "  71328,\n",
       "  72099,\n",
       "  74147,\n",
       "  74255,\n",
       "  74614,\n",
       "  76610,\n",
       "  76884,\n",
       "  77138,\n",
       "  77509,\n",
       "  77787,\n",
       "  77936,\n",
       "  78574,\n",
       "  80597,\n",
       "  81782,\n",
       "  82003,\n",
       "  84363,\n",
       "  84786,\n",
       "  85251,\n",
       "  89837,\n",
       "  97093,\n",
       "  98642,\n",
       "  100474,\n",
       "  105063,\n",
       "  105298,\n",
       "  105721,\n",
       "  108710,\n",
       "  109133,\n",
       "  112844,\n",
       "  113725,\n",
       "  114997,\n",
       "  115583,\n",
       "  115833,\n",
       "  116368,\n",
       "  116557,\n",
       "  121896,\n",
       "  124823,\n",
       "  126409,\n",
       "  126542,\n",
       "  128758,\n",
       "  130976,\n",
       "  131939,\n",
       "  132826,\n",
       "  136914,\n",
       "  137187,\n",
       "  137872,\n",
       "  139196,\n",
       "  140721,\n",
       "  142324,\n",
       "  146781,\n",
       "  151497,\n",
       "  154335,\n",
       "  155139,\n",
       "  155438,\n",
       "  155886,\n",
       "  156405,\n",
       "  158108,\n",
       "  159817,\n",
       "  160107,\n",
       "  161158,\n",
       "  162085,\n",
       "  165847,\n",
       "  168316,\n",
       "  168528,\n",
       "  169111,\n",
       "  170333,\n",
       "  172684,\n",
       "  182047,\n",
       "  182427,\n",
       "  186362,\n",
       "  189535,\n",
       "  190999,\n",
       "  191110,\n",
       "  193177,\n",
       "  196686,\n",
       "  202552,\n",
       "  206340,\n",
       "  207789,\n",
       "  208382,\n",
       "  209874,\n",
       "  210525,\n",
       "  217464,\n",
       "  219933,\n",
       "  221393,\n",
       "  221533,\n",
       "  221880,\n",
       "  222213,\n",
       "  224190,\n",
       "  229501,\n",
       "  229598,\n",
       "  230783,\n",
       "  231453,\n",
       "  232140,\n",
       "  234427,\n",
       "  236193,\n",
       "  236950,\n",
       "  240168,\n",
       "  241891,\n",
       "  242359,\n",
       "  242934,\n",
       "  243030,\n",
       "  244831,\n",
       "  249882,\n",
       "  251277,\n",
       "  251901,\n",
       "  256360,\n",
       "  260244,\n",
       "  261612,\n",
       "  262384,\n",
       "  263856,\n",
       "  266638,\n",
       "  269760,\n",
       "  270113,\n",
       "  270674,\n",
       "  271146,\n",
       "  271885,\n",
       "  272560,\n",
       "  273464,\n",
       "  275086,\n",
       "  275664,\n",
       "  276207,\n",
       "  277407,\n",
       "  292648,\n",
       "  299762,\n",
       "  306277,\n",
       "  307507,\n",
       "  307972,\n",
       "  308148,\n",
       "  308330,\n",
       "  308568,\n",
       "  311856,\n",
       "  317491,\n",
       "  321194,\n",
       "  321925,\n",
       "  328154,\n",
       "  328470,\n",
       "  328977,\n",
       "  330435,\n",
       "  331650,\n",
       "  337494,\n",
       "  340526,\n",
       "  348636,\n",
       "  349331,\n",
       "  350022,\n",
       "  350168,\n",
       "  350674,\n",
       "  350949,\n",
       "  351349,\n",
       "  354175,\n",
       "  354411,\n",
       "  356314,\n",
       "  356541,\n",
       "  356788,\n",
       "  357522,\n",
       "  358413,\n",
       "  359478,\n",
       "  360511,\n",
       "  362108,\n",
       "  363675,\n",
       "  364467,\n",
       "  365049,\n",
       "  366954,\n",
       "  367286,\n",
       "  367741,\n",
       "  368878,\n",
       "  369487,\n",
       "  373757,\n",
       "  377123,\n",
       "  378852,\n",
       "  379696],\n",
       " 'nils.txt': [991703]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_index['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your master index in a file and read it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "with open('master.idx', 'wb') as f:\n",
    "    pickle.dump(master_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gosta.txt': [(313784, 313790), (409998, 410004), (538165, 538171)],\n",
       " 'nils.txt': [(51805, 51811), (118943, 118949)],\n",
       " 'osynliga.txt': [(399121, 399127)],\n",
       " 'troll.txt': [(641880, 641886), (654233, 654239)]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('master.idx', 'rb') as f:\n",
    "    master_loaded = pickle.load(f)\n",
    "master_loaded['samlar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `concordance(word, master_index, window)` function to extract the concordances of a `word` within a window of `window` characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def concordance(word, master_index, window):\n",
    "    word = word.lower()\n",
    "    results = []\n",
    "    if word not in master_index:\n",
    "        return results\n",
    "    for file_name, positions in master_index[word].items():\n",
    "        file_path = os.path.join(corpus_folder, file_name)  # full path\n",
    "        with open(file_path, 'r', encoding='utf=8') as f:\n",
    "            \n",
    "            text = f.read()\n",
    "            for span in positions:\n",
    "                start_pos = span[0]\n",
    "                start = max(start_pos - window, 0)\n",
    "                end = min(start_pos + len(word)+ window, len(text))\n",
    "                context = text[start:end]\n",
    "                results.append((file_name, context))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gosta.txt', 'om ligger nära Borg, och samlar ihop ett litet middagssä'),\n",
       " ('gosta.txt', 'lika förstämda.\\n\\nMen hon samlar upp allt detta som glöda'),\n",
       " ('gosta.txt', 'n ensam i livet.\\n\\nDärmed samlar han korten tillhopa, res'),\n",
       " ('nils.txt', ' bara, att du i all hast samlar ihop så mycket boss och '),\n",
       " ('nils.txt', 'ar stannat hemma, och nu samlar de sig för att intränga '),\n",
       " ('osynliga.txt', ' till höger i kärran och samlar just ihop tömmarna, och '),\n",
       " ('troll.txt', 'en örtkunnig läkare, som samlar in markens växter för at'),\n",
       " ('troll.txt', 'älper dem, och medan hon samlar och handlar för deras rä')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concordance('samlar', master_index, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Documents with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the index, you will represent each document in your corpus as a dictionary. The keys of these dictionaries will be the words and you will define the value of a word with the tf-idf metric: \n",
    "1. Read the description of the tf-idf measure on Wikipedia (<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">https://en.wikipedia.org/wiki/Tf-idf</a>)\n",
    "2. After reading the description, you probably realized that there are multiple definitions of tf-idf. In this assignment, \n",
    " * Tf will be the relative frequency of the term in the document and \n",
    " * idf, the logarithm base 10 of the inverse document frequency.\n",
    "        \n",
    "You have below the tf-idf values for a few words. In our example, the word <i>gås</i> has the value 0 in bannlyst.txt and the value 0.000101001964 in nils.txt\n",
    "\n",
    "<pre>\n",
    "troll.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 2.148161748868631e-06\n",
    "\tet\t 0.0\n",
    "kejsaren.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 8.08284798629935e-06\n",
    "\tet\t 8.273225429362848e-05\n",
    "marbacka.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 7.582276564686669e-06\n",
    "\tet\t 9.70107989686256e-06\n",
    "herrgard.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "nils.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.00010100196417506702\n",
    "\tnils\t 0.00010164426900380124\n",
    "\tet\t 0.0\n",
    "osynliga.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "jerusalem.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 4.968292117670952e-06\n",
    "\tet\t 0.0\n",
    "bannlyst.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "gosta.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the tf-idf representation is a vector. In your program, you will keep this idea and use all the words in the corpus as keys: Each dictionary will include all the words of the corpus as keys. The value of the key is then possibly 0, meaning that the word in not in the document or is in all the documents as for the word `nils` in `gosta.tx`. \n",
    "\n",
    "As further work, you may think of optimizing this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import regex\n",
    "from collections import Counter\n",
    "\n",
    "vocab = set(master_index.keys())\n",
    "N = len(corpus_files)\n",
    "\n",
    "# Document frequency\n",
    "df = {word: len(posting) for word, posting in master_index.items()}\n",
    "\n",
    "tfidf = {}\n",
    "for file_name in corpus_files:\n",
    "    tfidf[file_name] = {}\n",
    "    \n",
    "    file_path = os.path.join(corpus_folder, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    \n",
    "    # Tokenize with Unicode regex\n",
    "    tokens = [m.group() for m in regex.finditer(r'\\p{L}+', text)]\n",
    "    \n",
    "    # Count term frequencies\n",
    "    counts = Counter(tokens)\n",
    "    \n",
    "    # Compute TF-IDF\n",
    "    for token in vocab:\n",
    "        tf = counts.get(token, 0)       # frequency of token in this doc\n",
    "        idf = math.log(N / df[token]) if df[token] > 0 else 0\n",
    "        tfidf[file_name][token] = tf * idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['troll.txt']['känna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5877866649021191"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['troll.txt']['nils']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cosine similarity, compare all the pairs of documents with their tf-idf representation and present your results in a table. You will include this table in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function computing the cosine similarity between two documents: `cosine_similarity(document1, document2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def cosine_similarity(document1, document2):\n",
    "    common_words = set(document1.keys()) & set(document2.keys())\n",
    "    dot_product = sum(document1[word] * document2[word] for word in common_words)\n",
    "    \n",
    "    norm1 = math.sqrt(sum(value**2 for value in document1.values()))\n",
    "    norm2 = math.sqrt(sum(value**2 for value in document2.values()))\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity matrix between the documents of the corpus. While computing the similarities, you will record the two most similar documents that you will call `most_sim_doc1` and `most_sim_doc2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               marbacka.txt   bannlyst.txt   jerusalem.txt  gosta.txt      nils.txt       herrgard.txt   osynliga.txt   troll.txt      kejsaren.txt   \n",
      "marbacka.txt             1.000          0.037          0.005          0.080          0.085          0.004          0.093          0.147          0.071\n",
      "bannlyst.txt             0.037          1.000          0.006          0.049          0.051          0.001          0.052          0.089          0.024\n",
      "jerusalem.txt            0.005          0.006          1.000          0.004          0.005          0.371          0.028          0.007          0.002\n",
      "gosta.txt                0.080          0.049          0.004          1.000          0.105          0.003          0.125          0.196          0.048\n",
      "nils.txt                 0.085          0.051          0.005          0.105          1.000          0.005          0.111          0.188          0.050\n",
      "herrgard.txt             0.004          0.001          0.371          0.003          0.005          1.000          0.005          0.004          0.001\n",
      "osynliga.txt             0.093          0.052          0.028          0.125          0.111          0.005          1.000          0.193          0.051\n",
      "troll.txt                0.147          0.089          0.007          0.196          0.188          0.004          0.193          1.000          0.181\n",
      "kejsaren.txt             0.071          0.024          0.002          0.048          0.050          0.001          0.051          0.181          1.000\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "docs = list(tfidf.keys())\n",
    "n = len(docs)\n",
    "matrix = [[0.0] * n for _ in range(n)]\n",
    "max_score = -1\n",
    "most_sim_docs1, most_sim_docs2 = None, None\n",
    "\n",
    "for i, doc1 in enumerate(docs):\n",
    "    for j, doc2 in enumerate(docs):\n",
    "        score = cosine_similarity(tfidf[doc1], tfidf[doc2])\n",
    "        matrix[i][j] = score\n",
    "        if i < j and score > max_score:\n",
    "            max_score = score\n",
    "            most_sim_docs1, most_sim_docs2 = doc1, doc2\n",
    "\n",
    "print(\" \" * 15, end=\"\")\n",
    "for doc in docs:\n",
    "    print(f\"{doc:15}\", end=\"\")\n",
    "print()\n",
    "\n",
    "# Print rows\n",
    "for i, doc1 in enumerate(docs):\n",
    "    print(f\"{doc1:15}\", end=\"\")\n",
    "    for j in range(n):\n",
    "        print(f\"{matrix[i][j]:15.3f}\", end=\"\")  # 3 decimals\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the name of the two novels that are the most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar: jerusalem.txt herrgard.txt Similarity: 0.371\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar:\", most_sim_docs1, most_sim_docs2, \"Similarity:\", round(max_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Turning in your assigment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will:\n",
    "1. Write a short individual report on your program, \n",
    "2. Read the text <i>Challenges in Building Large-Scale Information Retrieval Systems</i> about the history of <a href=\"https://research.google.com/people/jeff/WSDM09-keynote.pdf\">Google indexing</a> by <a href=\"https://research.google.com/pubs/jeff.html\">Jeff Dean</a> and write a short comment on it. See the details below.\n",
    "3. Watch the speech delivered by Steve Jobs in 1985 (https://www.youtube.com/watch?v=NT6oUsn3ANA) and describe superficially in one paragraph how computers can amass knowledge and have dialogues with us.\n",
    "\n",
    "You will submit your report as well as your notebook (for archiving purposes) to Canvas: <https://canvas.education.lu.se/>. You have two separate places: one of the report and the other for the program.\n",
    "\n",
    "I suggest that you use this structure for your report:\n",
    "1. Objectives and dataset\n",
    "2. Method and program structure, where you should outline your program and possibly describe difficult parts. \n",
    "3. Results\n",
    "4. Conclusion\n",
    "5. Answer to possible questions\n",
    "\n",
    "In your report of about two pages:\n",
    "1. You will describe your indexer (Sect. Method) and comment your results (Sect. Result); in this description, you will write the regular expression you used for the tokenization (Sect. Method) and include the similarity matrix (Sect. Results);\n",
    "2. In Jeff Dean's document, slide 45 describes an indexing technique. Tell how your index encoding is related to what Google did. (Sect. Other questions).\n",
    "3. Referring to Jobs's speech, comment shortly how computers can amass knowledge and have dialogues with us.\n",
    "\n",
    "To write your report, use Latex. This is mandatory as it will help you structure your text. You will then upload a PDF file in Canvas. You can try Overleaf (<a href=\"https://www.overleaf.com/\">www.overleaf.com</a>) as it makes it easy to create Latex document.\n",
    "\n",
    "The submission deadline is September 19, 2025. You will have only three submission attempts. The deadline for the second and third ones are one week after you are noticed of your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
